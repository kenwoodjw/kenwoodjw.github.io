{"posts":[{"title":"kubeflow国内环境最新安装方式","content":"最近又开始折腾kubeflow，发现以前用的kfctl 安装方式，官网github已经两年没更新，官方也推出了新的安装方式，但有些镜像是国外的，所以需要解决国外谷歌镜像拉取问题 获取镜像列表 官方安装脚本仓库 https://github.com/kubeflow/manifests 安装前准备 # 需要安装kustomize，kubectl，注意k8s版本兼容, https://github.com/kubeflow/manifests#prerequisites # StorageClass 可使用 https://github.com/rancher/local-path-provisioner 镜像同步至dockerhub方式 ### 获取gcr镜像，因为我的网络只无法获取gcr.io, quay.io正常，可以根据需求修改 kustomize build example |grep 'image: gcr.io'|awk '$2 != &quot;&quot; { print $2}' |sort -u ### 使用github-ci同步至个人dockerhub仓库 https://github.com/kenwoodjw/sync_gcr 修改https://github.com/kenwoodjw/sync_gcr/blob/master/images.txt 提交会触发ci同步镜像至dockerhub 可根据需求修改https://github.com/kenwoodjw/sync_gcr/blob/master/sync_image.py 修改安装脚本拉取镜像 #https://github.com/kubeflow/manifests/blob/master/example/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: # Cert-Manager - ../common/cert-manager/cert-manager/base - ../common/cert-manager/kubeflow-issuer/base # Istio - ../common/istio-1-14/istio-crds/base - ../common/istio-1-14/istio-namespace/base - ../common/istio-1-14/istio-install/base # OIDC Authservice - ../common/oidc-authservice/base # Dex - ../common/dex/overlays/istio # KNative - ../common/knative/knative-serving/overlays/gateways - ../common/knative/knative-eventing/base - ../common/istio-1-14/cluster-local-gateway/base # Kubeflow namespace - ../common/kubeflow-namespace/base # Kubeflow Roles - ../common/kubeflow-roles/base # Kubeflow Istio Resources - ../common/istio-1-14/kubeflow-istio-resources/base # Kubeflow Pipelines - ../apps/pipeline/upstream/env/cert-manager/platform-agnostic-multi-user # Katib - ../apps/katib/upstream/installs/katib-with-kubeflow # Central Dashboard - ../apps/centraldashboard/upstream/overlays/kserve # Admission Webhook - ../apps/admission-webhook/upstream/overlays/cert-manager # Jupyter Web App - ../apps/jupyter/jupyter-web-app/upstream/overlays/istio # Notebook Controller - ../apps/jupyter/notebook-controller/upstream/overlays/kubeflow # Profiles + KFAM - ../apps/profiles/upstream/overlays/kubeflow # Volumes Web App - ../apps/volumes-web-app/upstream/overlays/istio # Tensorboards Controller - ../apps/tensorboard/tensorboard-controller/upstream/overlays/kubeflow # Tensorboard Web App - ../apps/tensorboard/tensorboards-web-app/upstream/overlays/istio # Training Operator - ../apps/training-operator/upstream/overlays/kubeflow # User namespace - ../common/user-namespace/base # KServe - ../contrib/kserve/kserve - ../contrib/kserve/models-web-app/overlays/kubeflow images: - name: gcr.io/arrikto/istio/pilot:1.14.1-1-g19df463bb newName: kenwood/pilot newTag: &quot;1.14.1-1-g19df463bb&quot; - name: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef newName: kenwood/oidc-authservice newTag: &quot;28c59ef&quot; - name: gcr.io/knative-releases/knative.dev/eventing/cmd/controller@sha256:dc0ac2d8f235edb04ec1290721f389d2bc719ab8b6222ee86f17af8d7d2a160f newName: kenwood/cmd/controller newTag: &quot;dc0ac2&quot; - name: gcr.io/knative-releases/knative.dev/eventing/cmd/mtping@sha256:632d9d710d070efed2563f6125a87993e825e8e36562ec3da0366e2a897406c0 newName: kenwood/cmd/mtping newTag: &quot;632d9d&quot; - name: gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook@sha256:847bb97e38440c71cb4bcc3e430743e18b328ad1e168b6fca35b10353b9a2c22 newName: kenwood/domain-mapping-webhook newTag: &quot;847bb9&quot; - name: gcr.io/knative-releases/knative.dev/eventing/cmd/webhook@sha256:b7faf7d253bd256dbe08f1cac084469128989cf39abbe256ecb4e1d4eb085a31 newName: kenwood/webhook newTag: &quot;b7faf7&quot; - name: gcr.io/knative-releases/knative.dev/net-istio/cmd/controller@sha256:f253b82941c2220181cee80d7488fe1cefce9d49ab30bdb54bcb8c76515f7a26 newName: kenwood/controller newTag: &quot;f253b8&quot; - name: gcr.io/knative-releases/knative.dev/net-istio/cmd/webhook@sha256:a705c1ea8e9e556f860314fe055082fbe3cde6a924c29291955f98d979f8185e newName: kenwood/webhook newTag: &quot;a705c1&quot; - name: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:93ff6e69357785ff97806945b284cbd1d37e50402b876a320645be8877c0d7b7 newName: kenwood/activator newTag: &quot;93ff6e&quot; ..... 开始安装 while ! kustomize build example | kubectl apply -f -; do echo &quot;Retrying to apply resources&quot;; sleep 10; done ","link":"https://kenwoodjw.github.io/post/kubeflow-guo-nei-huan-jing-zui-xin-an-zhuang-fang-shi/"},{"title":"OpenSSH 用户枚举漏洞(CVE-2018-15473)修复","content":"升级openssh wget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.6p1.tar.gz tar xvf openssh-8.6p1.tar.gz ./configure yum -y group install &quot;Development Tools&quot; yum -y install openssl-devel make &amp;&amp;make install #备份 cp /usr/lib/systemd/system/sshd.service{,.bak} #验证 ll /usr/lib/systemd/system/sshd.service* #修改配置 cat &gt; /usr/local/etc/sshd_config &lt;&lt;EOF UseDNS no AddressFamily inet SyslogFacility AUTHPRIV PermitRootLogin yes PasswordAuthentication yes Banner /etc/ssh/ssh_banner EOF cat &gt;/usr/lib/systemd/system/sshd.service &lt;&lt;EOF [Unit] Description=OpenSSH server daemon [Service] ExecStart=/usr/local/sbin/sshd -f /usr/local/etc/sshd_config -D ExecReload=/bin/kill -HUP $MAINPID KillMode=process [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl restart sshd 验证 https://github.com/Rhynorater/CVE-2018-15473-Exploit pip3 install paramiko==2.4.1 python3 sshUsernameEnumExploit.py --port 20022 --userList exampleInput.txt your-ip ","link":"https://kenwoodjw.github.io/post/openssh-yong-hu-mei-ju-lou-dong-cve-2018-15473xiu-fu/"},{"title":"Loki","content":"loki 日志采集安装 下载loki和promtail wget https://github.com/grafana/loki/releases/download/v1.5.0/loki-linux-amd64.zip uznip loki-linux-amd64.zip mv loki-linux-amd64 /usr/local/bin/loki wget https://github.com/grafana/loki/releases/download/v1.5.0/promtail-linux-amd64.zip unzip promtail-linux-amd64.zip unzip promtail-linux-amd64 /usr/local/bin/promtail 配置文件 https://github.com/grafana/loki/blob/v1.5.0/cmd/loki/loki-local-config.yaml https://github.com/grafana/loki/blob/v1.5.0/cmd/promtail/promtail-local-config.yaml mv loki-local-config.yaml /usr/local/bin/config-loki.yml mv promtail-local-config.yaml /usr/local/bin/config-promtail.yml 配置systemd vi /etc/systemd/system/loki.service [Unit] Description=Loki service After=network.target [Service] Type=simple ExecStart=/usr/local/bin/loki -config.file /usr/local/bin/config-loki.yml [Install] WantedBy=multi-user.target vi /etc/systemd/system/promtail.service [Unit] Description=Promtail service After=network.target [Service] Type=simple ExecStart=/usr/local/bin/promtail -config.file /usr/local/bin/config-promtail.yml [Install] WantedBy=multi-user.target systemctl start loki systemctl enable loki systemctl start promtail systemctl start promtail 验证 [http://Your-Server-Domain-or-IP]:3100/metrics [http://Your-Server-Domain-or-IP]:9080 grafana 数据源添加 添加数据源 http://127.0.0.1:3100 点击查看exporter ","link":"https://kenwoodjw.github.io/post/loki/"},{"title":"Docker","content":"docker namespace 概念 首先让我们看看docker 官方文档的解释 当你启动一个container，docker会为container创建 一系列的namespaces 小测试 # 在容器里运行 watch 'ps ax' Every 2s: ps ax 2020-03-01 09:44:12 PID USER TIME COMMAND 1 root 0:03 /sbin/runsvdir -P /etc/service/enabled 64 root 0:00 runsv felix 65 root 0:00 runsv bird 66 root 0:00 runsv bird6 67 root 0:00 runsv confd 69 root 1h21 calico-node -felix 71 root 0:28 calico-node -confd 176 root 1:38 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg 177 root 1:26 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg 28692 root 0:00 sh 28720 root 0:00 watch ps ax 29670 root 0:00 ps ax # 查看 宿主机pid 21609 pts/0 S+ 0:00 watch ps ax # 同样的进程不同的pid # 查看进程树 pstree -p systemd(1)─┬─NetworkManager(974)─┬─{NetworkManager}(1034) │ └─{NetworkManager}(1036) ├─agetty(6593) ├─aksusbd_x86_64(31834)─┬─{aksusbd_x86_64}(31835) │ └─{aksusbd_x86_64}(31836) ├─auditd(934)───{auditd}(935) ├─chronyd(1003) ├─containerd-shim(1335)─┬─{containerd-shim}(1336) │ ├─{containerd-shim}(1337) │ ├─{containerd-shim}(1338) │ ├─{containerd-shim}(1339) │ ├─{containerd-shim}(1340) │ ├─{containerd-shim}(1341) │ ├─{containerd-shim}(1342) │ ├─{containerd-shim}(1343) │ └─{containerd-shim}(5137) ├─containerd-shim(1366)─┬─{containerd-shim}(1367) │ ├─{containerd-shim}(1368) │ ├─{containerd-shim}(1369) │ ├─{containerd-shim}(1372) │ ├─{containerd-shim}(1373) │ ├─{containerd-shim}(1374) │ ├─{containerd-shim}(1376) │ ├─{containerd-shim}(1378) │ └─{containerd-shim}(5139) ├─crond(1024) ├─dbus-daemon(967) ├─dockerd(1419)─┬─containerd(1433)─┬─containerd-shim(2424)─┬─pause(2498) │ │ │ ├─{containerd-shim}(2426) │ │ │ ├─{containerd-shim}(2427) │ │ │ ├─{containerd-shim}(2428) │ │ │ ├─{containerd-shim}(2430) │ │ │ ├─{containerd-shim}(2431) │ │ │ ├─{containerd-shim}(2432) │ │ │ ├─{containerd-shim}(2435) │ │ │ ├─{containerd-shim}(2438) │ │ │ ├─{containerd-shim}(2616) │ │ │ └─{containerd-shim}(5154) docker 启动流程 dockerd-&gt; containerd-&gt;containerd-shim-puase containerd containerd is available as a daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond. 容器化的运行时要求非常低。 与Linux和Windows容器功能集的大多数交互都是通过runc和/或特定于操作系统的库（例如，适用于Microsoft的hcsshim）处理的。 RUNC.md中始终列出了当前所需的runc版本。 runc 什么是runc runc is a CLI tool for spawning and running containers according to the OCI specification. 跟踪 strace -f -p `pidof containerd` -o strace_log # 查找关键词unshare unshare(CLONE_NEWNS|CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWPID|CLONE_NEWNET &lt;unfinished ...&gt; 什么是unshare 取消共享-运行具有父级未共享的某些名称空间的程序 http://man7.org/linux/man-pages/man1/unshare.1.html unshare/clone/syscalls: please fake pid for child&quot; ","link":"https://kenwoodjw.github.io/post/docker/"},{"title":"prometheus 使用blackbox-exporter","content":"Blackbox-exporter介绍 Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测。 实操 创建blackbox deployment，svc，configmap apiVersion: v1 kind: ConfigMap metadata: name: blackbox-config namespace: kube-mon data: blackbox.yml: |- modules: http_2xx: # http 检测模块 Blockbox-Exporter 中所有的探针均是以 Module 的信息进行配置 prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] valid_status_codes: [200] # 这里最好作一个返回状态码，在grafana作图时，有明示---陈刚注释。 method: GET preferred_ip_protocol: &quot;ip4&quot; http_post_2xx: # http post 监测模块 prober: http timeout: 10s http: valid_http_versions: [&quot;HTTP/1.1&quot;, &quot;HTTP/2&quot;] method: POST preferred_ip_protocol: &quot;ip4&quot; tcp_connect: # TCP 检测模块 prober: tcp timeout: 10s dns: # DNS 检测模块 prober: dns dns: transport_protocol: &quot;tcp&quot; # 默认是 udp preferred_ip_protocol: &quot;ip4&quot; # 默认是 ip6 query_name: &quot;kubernetes.default.svc.cluster.local&quot; --- apiVersion: apps/v1 kind: Deployment metadata: name: blackbox namespace: kube-mon spec: selector: matchLabels: app: blackbox template: metadata: labels: app: blackbox spec: containers: - image: prom/blackbox-exporter:v0.16.0 name: blackbox args: - --config.file=/etc/blackbox_exporter/blackbox.yml # ConfigMap 中的配置文件 - --log.level=error # 错误级别控制 ports: - containerPort: 9115 volumeMounts: - name: config mountPath: /etc/blackbox_exporter volumes: - name: config configMap: name: blackbox-config --- apiVersion: v1 kind: Service metadata: name: blackbox namespace: kube-mon spec: selector: app: blackbox type: NodePort ports: - port: 9115 targetPort: 9115 nodePort: 30010 增加prometheus配置（物理机版本) - job_name: 'kubernetes-http-services' metrics_path: /probe params: module: [http_2xx] # 使用定义的http模块 kubernetes_sd_configs: - role: service api_server: https://{{apiserver_address}} tls_config: insecure_skip_verify: true bearer_token: '{{bearer_token}}' tls_config: insecure_skip_verify: true bearer_token: '{{bearer_token}}' relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: 172.25.21.121:30010 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name 添加注释, 只有service的annotation中配置了 prometheus.io/http_probe=true 的才进行发现 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: redis namespace: kube-ops spec: template: metadata: labels: app: redis spec: containers: - name: redis image: redis:4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 - name: redis-exporter image: oliver006/redis_exporter:latest resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 9121 --- kind: Service apiVersion: v1 metadata: name: redis namespace: kube-ops annotations: prometheus.io/http-probe: &quot;true&quot; spec: selector: app: redis ports: - name: redis port: 6379 targetPort: 6379 - name: prom port: 9121 targetPort: 9121 可视化 面板id：9965， 7587 参考资料: https://www.qikqiak.com/post/blackbox-exporter-on-prometheus/ https://blog.fleeto.us/post/blackbox-monitor-dns-web/ https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml https://zhangguanzhang.github.io/2018/12/04/black-box-exporter/ ","link":"https://kenwoodjw.github.io/post/k8s_05/"},{"title":"K8s_04","content":"为什么需要ingress？ Ingress 只需要一个公网ip就能为许多服务提供访问，当客户端向ingress发送HTTP请求时，Ingress 会根据请求的主机名和路径决定请求转发到的服务 部署测试demo kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml 验证安装 kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx --watch POD_NAMESPACE=ingress-nginx POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].metadata.name}') kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version 参考link https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal apiVersion: v1 kind: Service metadata: name: ingress-nginx namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx spec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP - name: proxied-tcp-9000 port: 9000 targetPort: 9000 protocol: TCP selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx 测试http 7层负载 kubectl run test-hello --image=nginx:alpine --expose --port=80 ## # kubectl get deploy test-hello NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE test-hello 1 1 1 1 56s # kubectl get svc test-hello NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE test-hello ClusterIP 10.68.124.115 &lt;none&gt; 80/TCP 1m 然后为这个应用创建 ingress ``` # test-hello.ing.yaml内容 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-hello spec: rules: - host: hello.test.com http: paths: - path: / backend: serviceName: test-hello servicePort: 80 集群内部尝试访问: curl -H Host:hello.test.com 10.68.69.170(ingress-service的服务地址) 能够看到欢迎页面 Welcome to nginx!； TCP /UDP Ingress does not support TCP or UDP services. For this reason this Ingress controller uses the flags --tcp-services-configmap and --udp-services-configmap to point to an existing config map where the key is the external port to use and the value indicates the service to expose using the format: &lt;namespace/service name&gt;:&lt;service port&gt;:[PROXY]:[PROXY] apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: &quot;default/example-go:8080&quot; 参考官网 https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ ","link":"https://kenwoodjw.github.io/post/k8s_04/"},{"title":"K8s_03","content":"configmap 热更新 更新应用配置且不重启应用程序 使用环境变量或者命令行参数作为配置源的弊端在于无法在进程运行时更新配置。将configmap 暴露为卷可以达到配置热更新的效果，无须重新创建pod或者重启容器 了解更新configmap的影响 容器的一个比较重要的特性是其不变性，从同一镜像启动的多个容器之间不存在任何差异，那么修改被运行的容器所使用的configmap来打破这种不变性的行为是否是错误的？ 关键在于应用是否支持重载配置，configmap更新之后创建的pod也会使用新配置，而之前的pod依旧使用旧配置，这会导致运行中的不同实例的配置的不同，这不仅限于新pod， 如果pod中的容器因为某种原因重启了，新进程同样会使用新配置，因此，应用不支持主动重载配置，那么修改某些运行pod所使用的configmap并不是一个好主意 如果应用支持主动重载配置，那么修改configmap的行为就算不了什么。不过，不同的pod同步时间可能长达一分钟 ","link":"https://kenwoodjw.github.io/post/k8s_03/"},{"title":"K8s_02","content":"K8s 服务的创建 为什么需要服务？ 1.pod是短暂的，随时会启动或关闭 2.pod在启动时才分配地址，因此客户端不能提前知道服务的pod的ip地址 3.水平伸缩意味着多个pod提供相同服务，所有的pod都可以通过一个IP地址进行访问 实际测试 创建服务 apiVersion: v1 kind: Service metadata: name: kubia-nodeport spec: ports: - port: 80 targetPort: 8080 selector: app: kubia 创建一个名叫kubia-nodeport的服务，他在端口80接收请求并连接路由到具有标签选择器是app=kubia的pod的8080端口上 kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(s) AGE kubia 10.11.249.153 &lt;none&gt; 80/TCP 6m 分配的服务的IP的集群的IP，只能通过集群内部访问 从集群内部测试服务 kubectl exec kubia-7n0g1 -- curl -s http://10.11.249.153 服务发现 通过dns发现服务 集群中所有的pod都使用kube-dns或coredns 作为dns（k8s通过修改每个pod的 /etc/resolv.conf文件实现） tips：pod是否使用内部的DNS服务器是根据pod中spec的dnsPolicy属性决定的 通过FQDN连接服务 kubectl exec -it kubia-3inly bash root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local 服务IP无法ping通，因为服务的集群IP是虚拟IP，只有在与服务端口结合才有意义 ","link":"https://kenwoodjw.github.io/post/k8s_02/"},{"title":"Python装饰器","content":"python装饰器 python装饰传入一个函数，添加一些功能，然后返回它 闭包 先了解闭包的功能，对理解装饰器有很大的帮助 什么是闭包(closure),如何捕获变量 def dog(): height = 40 def profile(): print(&quot;I'm a dog and my height is {}&quot;.format(height)) return profile if __name__ == &quot;__main__&quot;: dog_profile = dog() dog_profile() function dog()里面包了一个和profile()function，在调用的dog()会返回profile,在一般认知中，function中的 variable其life cycle(生命周期)会随函数执行完而消灭，理论上变量height在执行完function dog()后就要消失，但在dog_profile()(调用profile())时还能够找到variable height，原因就是return profile时，函数profile capture 住了variable,把属于上一层的变量偷渡到自己函数范围,带有 capture variable的函数就是闭包. capture variable不能被assign def dog(): height = 40 def grow_up(): height = height +1 return grow_up if __name__ == &quot;__main__&quot;: dog_grow_up = dog() dog_grow_up() 报错UnboundLocalError，一般用global声明变量，如果在某个函数中同样命名的变量要赋值的话，一样会报UnboundLocalError，在python中，对变量赋值就等于建立局部变量 global x x = 10 def add_x(): x = x +1 if __name__ == &quot;__main__&quot;: add_x() 如何赋值capture variable def dog(): height = 40 def grow_up(): nonlocal height height = height +1 print(&quot;Thanks for making me growing up.I'm now {} meters!!!&quot;.format(height)) return grow_up if __name__==&quot;__main__&quot;: dog_grow_up = dog() dog_grow_up() 什么是Decorator(装饰器) # defining a decorator def hello_decorator(func): # inner1 is a Wrapper function in # which the argument is called # inner function can access the outer local # functions like in this case &quot;func&quot; def inner1(): print(&quot;Hello, this is before function execution&quot;) # calling the actual function now # inside the wrapper function. func() print(&quot;This is after function execution&quot;) return inner1 # defining a function, to be called inside wrapper def function_to_be_used(): print(&quot;This is inside the function !!&quot;) # passing 'function_to_be_used' inside the # decorator to control its behavior function_to_be_used = hello_decorator(function_to_be_used) # calling the function function_to_be_used() Output: Hello, this is before function execution This is inside the function !! This is after function execution {{&lt; figure src=&quot;/images/decorators_step.png&quot; width=&quot;&quot; height=&quot;&quot; &gt;}} Decorator的执行顺序 def print_func_name(func): def warp_1(): print(&quot;Now use function '{}'&quot;.format(func.__name__)) func() return warp_1 def print_time(func): import time def warp_2(): print(&quot;Now the Unix time is {}&quot;.format(int(time.time()))) func() return warp_2 @print_func_name @print_time def dog_bark(): print(&quot;Bark !!!&quot;) if __name__ == &quot;__main__&quot;: dog_bark() # &gt; Now use function 'warp_2' # &gt; Now the Unix time is 16532445 # &gt; Bark !!! dog_bark会先被@print_time吃进去，然后吐出一个warp_2的function，然后这个warp_2的function 又会被 print_func_name吃进去，返回一个叫做warp_1的function class式的Decorator class myDecorator: def __init__(self,fn): print(&quot;inside myDecorator.__init__()&quot;) self.fn = fn def __call__(self): self.fn() print(&quot;inside myDecorator.__call__()&quot;) @myDecorator def aFunction(): print(&quot;inside aFunction()&quot;) print(&quot;Finished decorating aFunction()&quot;) # 输出 # inside myDecorator.__init__() # Finished decorating aFunction() # inside aFunction() # inside myDecorator.__call__() Decorator的副作用 被decoratoe的函数其实已经是另一个函数了，如果你查询一下function.__name__的输出是&quot;wrapper&quot;. 这会给程序埋坑，python的functool包中提供一个叫wrap的decorator来消除副作用 from functools import wraps def hello(fn): @wraps(fn) def wrapper(): print(&quot;hello, %s&quot;%fn.__name__) fn() print(&quot;goodby, %s&quot;%fn.__name__) return wrapper @hello def foo(): '''foo help doc''' print(&quot;I am foo&quot;) pass foo() print(foo.__name__) print(foo.__doc__) 参考文章 https://coolshell.cn/articles/11265.html https://medium.com/citycoddee/python進階技巧-3-神奇又美好的-decorator-嗷嗚-6559edc87bc0 ","link":"https://kenwoodjw.github.io/post/python_03/"},{"title":"如何讲明白技术","content":"在面试的时候，技术问题所占的比重非常大，如何讲明白技术，决定了面试的成败。从某种程度，也体现你对技术的了解程度，所以如何讲明白技术。如何在学习的时候，对技术知识点进行加工内化，以下分析角度来源极客时间专栏：面试现场，我会尝试从这个角度去写的我技术文章 外部应用维度 问题 首先考虑的是要解决什么问题，这是技术产生的原因。Java 多线程的产生，是因为要并发，并发使得程序的多种功能响应更快，用户体验更好。问题这层就是回答干什么用 技术规范 怎么用这门技术，技术使用说明书 最佳实践 把技术应用到不同的场景时，发发现同样的使用方法，会有不同的效果。该技术有不同的适应面。从而你可能踩了很多坑，从中总结出最佳实践 市场应用趋势 随着技术生态的发展，和应用问题的变迁，技术的应用场景和流行趋势会受到影响。对于java，从低并发逐渐发展到高并发。谁在用，用在哪 设计维度 应用维度是从外部看技术的应用，那么从内部能看到技术的哪些层面？ 目标 为了解决用户的问题，技术本身要达成什么目标。比如，java多线程在优先级调度、锁、信息同步等方面达成怎样的目标，才能更好地实现并非 实现原理 为了达到设计目标，该技术采用了什么原理和机制。java多线程的实现原理包括内核线程，使用用户态线程加轻量级进程混合等。实现原理层回答怎么做到的问题。把实现原理弄懂，并且讲清楚，是技术人员的基本 优劣局限 每种技术实现，都有其局限性，在某些条件下能最大化的发挥效能，缺少某些条件则暴露出其缺陷。比如在java多线程编程中，采用共享内存的方式，锁的开销比较大，程序员编程难度大。 优劣局限层回答“做得怎么样的问题。对技术优劣局限的把握，更有利于应用时总结最佳实践，是分析各种坑的基础 演进趋势 技术是在迭代改进和不断淘汰的。了解技术的前生后世，分清技术不变的本质，和变化的脉络，以及与其他技术的共生关系，能体现你对技术发展趋势的关注和思考。 ","link":"https://kenwoodjw.github.io/post/blog01/"},{"title":"Arch_linux deepin 桌面蓝牙连接故障解决","content":"arch linux deepin桌面蓝牙无法连接 重装桌面deepin，发现没有蓝牙功能，按照wikihttps://wiki.archlinux.org/index.php/Bluetooth_(简体中文) 安装bluetooth yaourt -S pulseaudio-alsa pulseaudio-bluetooth bluez bluez-libs bluez-utils 启动bluetooth sudo systemctl start bluetooth sudo systemctl enable bluetooth 移除设备 bluetoothctl ➜ my-website git:(master) ✗ bluetoothctl Agent registered [F9-5.0]# devices Device 40:EF:4C:61:xx:xx EDIFIER R1700BT Device 34:36:3B:D1:xx:xx 34-36-3B-D1-xx-xx Device 76:84:EA:CB:xx:xx 76-84-EA-CB-xx-xx Device CB:4F:51:12:xx:xx Mi Band 3 remove xx:xx... 设备连接不了，日志里有错误 如果你在连接设备时看见 journalctl 的输出有类似的消息： 或者 sudo systemctl status bluetooth a2dp-source profile connect failed for 9C:64:40:22:E1:3F: Protocol not available 解决方法 安装 pulseaudio-bluetooth 再重启 PulseAudio。这个错误在文件传输是也会有。 killall pulseaudio pulseaudio --start --log-target=syslog ","link":"https://kenwoodjw.github.io/post/arch_linux_01/"},{"title":"Python后端架构演进","content":"转载自https://zhu327.github.io/2018/07/19/python%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/ 来腾讯之前在前公司做了3年的后端开发, 经历一款SaaS产品从0到10(还没有到100, 哈哈哈)的过程, 3年间后端的架构逐步演变, 在微服务的实践过程中遇到的问题也越来越多, 在这里总结下. 产品是一款服务于人力资源的SaaS在线服务, 面向HR有Web Android/iOS 小程序多个客户端, 后端采用RESTful风格API来提供服务. 主要使用Python语言, 方便快速迭代. 架构的演进经历了4个大的阶段: 1. MVC 2. 服务拆分 3. 微服务架构 4. 领域驱动设计. MVC 项目刚开始的时候, 后端同事不超过5个, 这个阶段主要的工作是实现产品的原型, 没有太多的考虑架构, 使用Django来快速实现功能, DB的表结构设计好之后, 抽象出功能View, 由于产品设计也很不完善, 后端需要很多的预留设计, 避免产品逻辑的变更带来整个表结构的变动, 在这个阶段代码上最重要的是确定适合团队的代码规范, 代码检查规则. 整体上架构如上图, Nginx负责负载均衡, 分发流量到多个Django服务, Django处理逻辑, 需要异步任务就交给Celery, 然后数据量比较大的地方使用Redis做缓存. 同时还有实时消息通知的需要使用了Nginx Push Module. 问题与优化方式: Django并发性能差 使用uWSGI Master+Worker 配合 gevent 携程支持高并发 Redis连接数过多 使用redis-py自带的连接池来实现连接复用 MySQL连接数过多 使用djorm-ext-pool连接池复用连接 Celery配置gevent支持并发任务 随着开发的功能越来越多, Django下的app也越来越多, 这就带了发布上的不方便, 每次发布版本都需要重启所有的Django服务, 如果发布遇到问题, 只能加班解决了. 而且单个Django工程下的代码量也越来越多, 不好维护. 服务拆分 随着后端团队的壮大, 分给每个同事的需求也越来越细, 如果继续在一个工程里面开发所有的代码, 维护起来的代价太高, 而我们的上一个架构中在Django里面已经按模块划分了一个个app, app内高类聚, app之间低耦合, 这就为服务的拆分带来了便利. 拆分的过程没有遇到太大的问题, 初期的拆分只是代码的分离, 把公用的代码抽离出来实现一个公用的Python库, 数据库, Redis还是共用, 随着负载的增加, 数据库也做了多实例. 如上图, 服务之间尽量避免相互调用, 需要交互的地方采用http请求的方式, 内网的调用使用hosts指向内网地址. 问题与优化方式: Nginx Push Module由于长时间没有维护, 长连接最大数量不够, 使用Tornado + ZeroMQ实现了tormq服务来支撑消息通知 服务之间的调用采用http的方式, 并且要求有依赖的服务主机配置hosts指向被调用的地址, 这样带来的维护上的不方便. 以及在调用链的过程中没有重试, 错误处理, 限流等等的策略, 导致服务可用性差. 随着业务拆分, 继续使用Nginx维护配置非常麻烦, 经常因为修改Nginx的配置引发调用错误. 每一个服务都有一个完整的认证过程, 认证又依赖于用户中心的数据库, 修改认证时需要重新发布多个服务 微服务架构 首先是在接入层引入了基于OpenResty的Kong API Gateway, 定制实现了认证, 限流等插件. 在接入层承接并剥离了应用层公共的认证, 限流等功能. 在发布新的服务时, 发布脚本中调用Kong admin api注册服务地址到Kong, 并加载api需要使用插件. 为了解决相互调用的问题, 维护了一个基于gevent+msgpack的RPC服务框架doge, 借助于etcd做服务治理, 并在rpc客户端实现了限流, 高可用, 负载均衡这些功能. 在这个阶段最难的技术选型, 开源的API网关大多用Golang与OpenResty(lua)实现, 为了应对我们业务的需要还要做定制. 前期花了1个月时间学习OpenResty与Golang, 并使用OpenResty实现了一个短网址服务shorturl用在业务中. 最终选择Kong是基于Lua发布的便利性, Kong的开箱即用以及插件开发比较容易. 性能的考量倒不是最重要的, 为了支撑更多的并发, 还使用了云平台提供的LB服务分发流量到2台Kong服务器组成的集群. 集群之间自动同步配置. 饿了么维护一个纯Python实现的thrift协议框架thriftpy, 并提供很多配套的工具, 如果团队足够大, 这一套RPC方案其实是合适的, 但是我们的团队人手不足, 水平参差不齐, 很难推广这一整套学习成本高昂的方案. 最终我们开发了类Duboo的RPC框架doge, 代码主要参考了weibo开源的motan. 领域驱动设计 在这一架构中我们尝试从应用服务中抽离出数据服务层, 每一个数据服务包含一个或多个界限上下文, 界限上下文类只有一个聚合根来暴露出RPC调用的方法. 数据服务不依赖于应用服务, 应用服务可以依赖多个数据服务. 有了数据服务层, 应用就解耦了相互之间的依赖, 高层服务只依赖于底层服务. 在我离职时领域驱动设计还在学习设计阶段, 还没有落地, 但是我相信前公司的后端架构一定会往这个方向继续演进. 总结 架构的设计, 技术的选型, 不能完全按照流行的技术走, 最终还是服务于产品, 服务于客户的需求. 设计过程中由于团队, 人员的结构问题, 有很多的妥协之处, 如何在妥协中找到最优解才是最大的挑战. Service Mesh这种新一代的微服务架构正在成为主流, 虽然现在的工作与微服务无关了, 但是也还会继续关注学习. ","link":"https://kenwoodjw.github.io/post/python_02/"},{"title":"为什么需要pod","content":"Pod的实现原理： 首先，关于Pod最重要的一个事实是：它只是一个逻辑概念。Pod，其实是一组共享了某些资源的容器。具体的说，Pod里的所有容器，共享的是同一个Network Namespace，并且可以声明共享同一个Volume。在kubernetes项目里，Pod的实现需要使用一个中间容器，这个容器叫做Infra容器（pause)，则通过Join Network Namespace的方式,与Infra容器关联在一起。 明白了Pod的实现原理后，我们再来讨论“容器设计模式”，就容易多了。Pod这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个Pod里的多个容器。 第一个最典型的例子是：war包与web服务器 我们现在有一个Java Web应用的WAR包，它需要被放在Tomcat的webapps目录下运行起来。 假如用docker来做这件事情 *.一种方法是，把WAR包直接放Tomcat镜像的webapps目录下，做成一个新的镜像运行起来，可是，如果你要更新WAR包的内容或要升级Tomcat镜像，就要重新制作一个新的发布镜像，非常麻烦 *.另一种方法是，你不管WAR包，永远只发布一个Tomcat容器，不过这个容器的webapps目录，就必须声明一个hostPath类型的Volume，从而把宿主机上的WAR包挂载进Tomcat容器中运行起来。不过，这样你就必须要解决一个问题，即：如何让每一台宿主机，都预先准备好这个存储WAR包的目录呢？这样来看，你只能独立维护一套分布式存储系统 iisCJKLanguage 实际上，有了Pod之后，我们可以把WAR包和Tomcat分别做成镜像，然后把它们作为一个Pod的两个容器“组合”在一起。 apiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: initContainers: - image: geektime/sample:v2 name: war command: [&quot;cp&quot;,&quot;/sample.war&quot;,&quot;/app&quot;] volumeMounts: - mountPath: /app name: app-volume containers: - image: geektime/tomcat:7.0 name: tomcat command: [&quot;sh&quot;,&quot;-c&quot;,&quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh&quot;] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001 volumes: - name: app-volume emptyDir: {} war包容器的类型不再是一个普通容器，而是一个Init Container类型的容器。在Pod中，所有Init Container定义的容器，都会比spec.containers定义的用户容器先启动。并且，Init Containers容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。 所以，这个Init Container类型的WAR包容器启动后，我执行了一句&quot;cp /sample.war/app&quot;,把应用的WAR包拷贝到/app目录下，然后退出。而后这个/app目录，就挂载了一个名叫app-volume的Volume。Tomcat容器，同样声明了挂载app-volume到自己的webapps目录下。所以，等Tomcat容器启动时，它的webapps目录下就一定会存在sample.war文件:这个文件正是WAR包容器启动时拷贝到这个Volume里面的，而这个Volume是被这两人容器共享的。 像这样，我们用一种”组合“的方式，解决了WAR包与Tomcat容器之间耦合关系的问题。 实际上，这个所谓的”组合“操作，正是容器设计模式里最常用的一种模式，它的名字叫： sidecar,顾名思义，sidecar指的就是我们可以在一个Pod中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。 比如，在我们的这个应用Pod中，Tomcat容器是我们要使用的主容器，而WAR包容器的存在，只是为了给它提供一个WAR包而已。所以，我们用Init Container的方式优先运行WAR包容器，扮演了一个sidewar的角色。 第二个例子，则是容器的日志收集。 比如，我现在有一个应用，需要不断地把日志文件输出到容器的/var/log目录中。 这时，我就可以把一个Pod里的Volume挂载到应用容器的/var/log目录上。 然后，我在这个Pod里同时运行一个sidecar容器，它是声明挂载同一个Volume到自己的/var/log目录上。 这样，接下来sidecar容器就只需要做一件事，那就是不断从自己的/var/log目录里读取日志文件，转发到MongoDB或者Elasticsearch中存储起来，这样，一个最基本的日志收集工作就完成了 不要忘记，Pod的另一个重要特性是，它的所有容器都共享同一个Network Namespace。这就使得很多与Pod网络相关的配置和管理，也都可以交给sidecar完成，而完全无须干涉用户容器，这里最典型的例子莫过于lstio这个微服务治理项目。 ","link":"https://kenwoodjw.github.io/post/k8s01/"},{"title":"每天一个python标准库00","content":"itertools --- 为高效循环而创建迭代器的函数 In [2]: dir(itertools) Out[2]: ['__doc__', '__loader__', '__name__', '__package__', '__spec__', '_grouper', '_tee', '_tee_dataobject', 'accumulate', 'chain', 'combinations', 'combinations_with_replacement', 'compress', 'count', 'cycle', 'dropwhile', 'filterfalse', 'groupby', 'islice', 'permutations', 'product', 'repeat', 'starmap', 'takewhile', 'tee', 'zip_longest'] 无穷迭代器 from itertools import count [i for i in count(10)] -- &gt; [10,11,12,13,14....] ","link":"https://kenwoodjw.github.io/post/python_standard_lib_01/"},{"title":"hugo部署网站，circleci自动构建静态页面","content":"1. 安裝 Hugo brew install hugo 2. 建立新网站 hugo new site my-website cd my-website git init my-website 可以替换成任意名称，但你也可以跟我一样，避免麻煩。找到名为 my-website 的文件夾，观察文件夹结构。 3. 新增主題 (theme)：此处以 Casper 为例 git submodule add git@github.com:vjeantet/hugo-theme-casper.git themes/casper 你也可以选其他主題，进到该主題的 GitHub repo，將上面的网址改成 repo 的网址、themes/casper 改成 themes/你的主題名称。 4. 將 /themes/casper 中的 static 和 layouts 文件夹复制，取代根目录中的 static 和 layouts 文件夹 5. 建立新文章 hugo new posts/my-first-post.md 此指令会在 /content/posts 文件夾中建立 my-first-post.md。使用任意文字编輯器打开此 markdown 文件，將 draft 改成 false，文件內容任意。以后建立文章都是以此方式。 使用文字编輯器打开 config.toml，并调整內容为上面这样。 6. 编輯 config.toml baseURL = &quot;https://kenwoodjw.github.io/&quot; #改成你的GitHub帳號名稱 languageCode = &quot;zh-cn&quot; title = &quot;kenwood Blog&quot; #自由命名 7. 本地测试 hugo server -D 执行完后，在浏览器中輸入网址 http://localhost:1313/ ，就可以看到网站的雏形了！ 第二部分：部署到 GitHub Hugo 部署在 GitHub 的方式有两种，这边介绍部署个人唯一主页的方式。 1. 在 GitHub 建立两个 repo 在 GitHub 建立 my-website 和 kenwoodjw.github.io (改成自己的帐号名称) 兩個 repo。 2. 建立 /public 文件夹 hugo 执行 hugo，就会在 my-website 內产生名为 public 的文件夾。 3. 将 /public 文件夾连接到 GitHub 上的 kenwoodjw.github.io #建立submodule git submodule add git@github.com:&lt;your-account&gt;/&lt;your-account&gt;.github.io.git public #上传submodule cd public git status git commit -m &quot;first commit&quot; git remote add origin git@github.com:&lt;you-account&gt;/&lt;your-account&gt;.github.io.git git push -u origin master 4. 将整个 my-website文件夹连接到 GitHub 上的 website-hugo cd ../ git remote add origin https://github.com/kenwood/my-website.git git add . git commit -m &quot;Initial commit&quot; git push -u origin master 在执行 git add . 时出现如下面的 warning 时不用在意 等个几分钟，输入网址 https://kenwoodjw.github.io/ 就大功告成！ 5.自动构建静态页面并上传到git 使用circleci，打开https://circleci.com链接github仓库，在项目顶层，新建.circleci/config.yml version: 2 global: &amp;global working_directory: ~/project docker: - image: cibuilds/hugo:latest jobs: build_n_deploy: &lt;&lt;: *global steps: - run: apk update &amp;&amp; apk add git - checkout - run: name: &quot;Run Hugo&quot; command: | git submodule sync &amp;&amp; git submodule update --init git submodule foreach --recursive git pull origin master HUGO_ENV=production hugo -v - run: name: &quot;Deploy to github.io&quot; command: | cd ~/project/public git config --global user.email &quot;xxxgmail.com&quot; git config --global user.name &quot;xxx&quot; git add -A git commit -m &quot;Deploy from CircleCI&quot; git push origin HEAD:master workflows: version: 2 build-deploy: jobs: - build_n_deploy: filters: branches: only: master References: Hugo Documentation 用Hugo搭建个人网站 Github个人网站维护 jimmylin blog ","link":"https://kenwoodjw.github.io/post/hugo/"}]}